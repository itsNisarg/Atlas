{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bc3244e6",
   "metadata": {},
   "source": [
    "# RL implementation of ATLAS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "070a3c88",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from collections import deque\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "313caf78",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ATLASGame:\n",
    "    def __init__(self, places):\n",
    "        self.places = sorted([p.upper() for p in places])\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.used_places = set()\n",
    "        self.current_place = None\n",
    "        self.current_player = 0  # 0 = agent, 1 = opponent\n",
    "        self.game_history = []\n",
    "        return self\n",
    "\n",
    "    def get_valid_moves(self):\n",
    "        if self.current_place is None:\n",
    "            return [p for p in self.places if p not in self.used_places]\n",
    "        last_letter = self.current_place[-1].upper()  # Added .upper()\n",
    "        return [p for p in self.places if p not in self.used_places and p[0].upper() == last_letter]  # Added .upper()\n",
    "\n",
    "    def make_move(self, place):\n",
    "        place = place.strip().upper()\n",
    "        if place not in self.places:\n",
    "            return False, f\"{place} is invalid\"\n",
    "        if place in self.used_places:\n",
    "            return False, f\"{place} already used\"\n",
    "\n",
    "        valid_moves = self.get_valid_moves()\n",
    "        if self.current_place is not None and place not in valid_moves:\n",
    "            return False, f\"{place} not valid now\"\n",
    "\n",
    "        self.used_places.add(place)\n",
    "        self.current_place = place\n",
    "        self.game_history.append((self.current_player, place))\n",
    "\n",
    "        # Switch players\n",
    "        self.current_player = 1 - self.current_player\n",
    "        return True, \"Move accepted\"\n",
    "\n",
    "    def is_terminal(self):\n",
    "        return len(self.get_valid_moves()) == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2714ae87",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AtlasStateEncoder:\n",
    "    def __init__(self, game):\n",
    "        self.game = game\n",
    "        self.n_places = len(game.places)\n",
    "        self.letter_to_idx = {chr(i+65): i for i in range(26)}  # A-Z\n",
    "\n",
    "    def encode(self):\n",
    "        state = []\n",
    "\n",
    "        # 1. Next required letter (one-hot)\n",
    "        letter_vec = np.zeros(26, dtype=np.float32)\n",
    "        if self.game.current_place:\n",
    "            last_letter = self.game.current_place[-1].upper()\n",
    "            letter_vec[self.letter_to_idx[last_letter]] = 1.0\n",
    "        state.extend(letter_vec)\n",
    "\n",
    "        # 2. Places used (binary)\n",
    "        used_vec = np.array([1.0 if p in self.game.used_places else 0.0 for p in self.game.places], dtype=np.float32)\n",
    "        state.extend(used_vec)\n",
    "\n",
    "        # 3. Valid moves mask (binary)\n",
    "        valid_moves = self.game.get_valid_moves()\n",
    "        valid_vec = np.array([1.0 if p in valid_moves else 0.0 for p in self.game.places], dtype=np.float32)\n",
    "        state.extend(valid_vec)\n",
    "\n",
    "        return np.array(state, dtype=np.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "de25f146",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AtlasEnv:\n",
    "    def __init__(self, places):\n",
    "        self.game = ATLASGame(places)\n",
    "        self.encoder = AtlasStateEncoder(self.game)\n",
    "        self.n_actions = len(places)\n",
    "\n",
    "    def reset(self):\n",
    "        self.game.reset()\n",
    "        return self.encoder.encode()\n",
    "\n",
    "    def step(self, action_idx):\n",
    "        done = False\n",
    "        reward = -0.01  # step penalty\n",
    "\n",
    "        place = self.game.places[action_idx]\n",
    "        success, msg = self.game.make_move(place)\n",
    "        if not success:\n",
    "            return self.encoder.encode(), -1.0, True\n",
    "\n",
    "        if self.game.is_terminal():\n",
    "            return self.encoder.encode(), 1.0, True\n",
    "\n",
    "        # Opponent random move\n",
    "        if self.game.current_player == 1:\n",
    "            opp_moves = self.game.get_valid_moves()\n",
    "            if opp_moves:\n",
    "                opp_place = random.choice(opp_moves)\n",
    "                self.game.make_move(opp_place)\n",
    "\n",
    "        if self.game.is_terminal():\n",
    "            return self.encoder.encode(), -0.5, True\n",
    "\n",
    "        return self.encoder.encode(), reward, done\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6512c88d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(state_dim, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, action_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "df1e4a18",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity=100_000):\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "\n",
    "    def push(self, s, a, r, s2, d):\n",
    "        self.buffer.append((s, a, r, s2, d))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        batch = random.sample(self.buffer, batch_size)\n",
    "        s, a, r, s2, d = zip(*batch)\n",
    "        return (\n",
    "            torch.from_numpy(np.array(s)).float(),\n",
    "            torch.from_numpy(np.array(a)).long(),\n",
    "            torch.from_numpy(np.array(r)).float(),\n",
    "            torch.from_numpy(np.array(s2)).float(),\n",
    "            torch.from_numpy(np.array(d)).float()\n",
    "        )\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "854c4d4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(path, model, target, optimizer, episode, epsilon):\n",
    "    torch.save({\n",
    "        \"model\": model.state_dict(),\n",
    "        \"target\": target.state_dict(),\n",
    "        \"optimizer\": optimizer.state_dict(),\n",
    "        \"episode\": episode,\n",
    "        \"epsilon\": epsilon,\n",
    "    }, path)\n",
    "\n",
    "\n",
    "def load_checkpoint(path, model, target, optimizer, device):\n",
    "    checkpoint = torch.load(path, map_location=device)\n",
    "\n",
    "    model.load_state_dict(checkpoint[\"model\"])\n",
    "    target.load_state_dict(checkpoint[\"target\"])\n",
    "    optimizer.load_state_dict(checkpoint[\"optimizer\"])\n",
    "\n",
    "    start_episode = checkpoint[\"episode\"] + 1\n",
    "    epsilon = checkpoint[\"epsilon\"]\n",
    "\n",
    "    return start_episode, epsilon\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8454ca23",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    CKPT_PATH = \"checkpoints/atlas_dqn_checkpoint.pt\"\n",
    "    places = pd.read_csv(\"../../data/countries.csv\")[\"Country\"].values.tolist()\n",
    "\n",
    "    env = AtlasEnv(places)\n",
    "    state_dim = len(env.reset())\n",
    "    action_dim = env.n_actions\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(\"Using device:\", device)\n",
    "\n",
    "    model = DQN(state_dim, action_dim).to(device)\n",
    "    target = DQN(state_dim, action_dim).to(device)\n",
    "    target.load_state_dict(model.state_dict())\n",
    "\n",
    "    optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "    buffer = ReplayBuffer()\n",
    "    gamma = 0.99\n",
    "    batch_size = 64\n",
    "\n",
    "    epsilon = 1.0\n",
    "    epsilon_min = 0.005\n",
    "    epsilon_decay = 0.9995\n",
    "    start_episode = 1\n",
    "\n",
    "    if os.path.exists(CKPT_PATH):\n",
    "        start_episode, epsilon = load_checkpoint(CKPT_PATH, model, target, optimizer, device)\n",
    "        print(f\"Resuming from episode {start_episode}, epsilon={epsilon:.3f}\")\n",
    "\n",
    "    for episode in range(start_episode, start_episode + 501):\n",
    "        state = env.reset()\n",
    "        done = False\n",
    "        total_reward = 0\n",
    "\n",
    "        while not done:\n",
    "            if random.random() < epsilon:\n",
    "                action = random.randrange(action_dim)\n",
    "            else:\n",
    "                with torch.no_grad():\n",
    "                    q = model(torch.tensor(state, dtype=torch.float32, device=device))\n",
    "                    valid_mask = torch.tensor(state[-action_dim:], device=device)\n",
    "                    q[valid_mask == 0] = -1e9\n",
    "                    action = torch.argmax(q).item()\n",
    "\n",
    "            next_state, reward, done = env.step(action)\n",
    "            buffer.push(state, action, reward, next_state, done)\n",
    "\n",
    "            state = next_state\n",
    "            total_reward += reward\n",
    "\n",
    "            if len(buffer) >= batch_size:\n",
    "                s, a, r, s2, d = buffer.sample(batch_size)\n",
    "                s = s.to(device, dtype=torch.float32)\n",
    "                a = a.to(device)\n",
    "                r = r.to(device)\n",
    "                s2 = s2.to(device, dtype=torch.float32)\n",
    "                d = d.to(device)\n",
    "\n",
    "                q_vals = model(s).gather(1, a.unsqueeze(1)).squeeze(1)\n",
    "                with torch.no_grad():\n",
    "                    q2 = target(s2).max(1)[0]\n",
    "                    target_q = r + gamma * q2 * (1 - d.float())\n",
    "\n",
    "                loss = F.smooth_l1_loss(q_vals, target_q)\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "                optimizer.step()\n",
    "\n",
    "        epsilon = max(epsilon_min, epsilon * epsilon_decay)\n",
    "\n",
    "        if episode % 100 == 0:\n",
    "            target.load_state_dict(model.state_dict())\n",
    "            print(f\"Episode {episode}, reward {total_reward:.2f}, epsilon {epsilon:.3f}\")\n",
    "\n",
    "        if episode % 500 == 0:\n",
    "            target.load_state_dict(model.state_dict())\n",
    "            save_checkpoint(CKPT_PATH, model, target, optimizer, episode, epsilon)\n",
    "            print(f\"Checkpoint saved at episode {episode}\")\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4379fa35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Resuming from episode 19001, epsilon=0.005\n",
      "Episode 19100, reward -0.58, epsilon 0.005\n",
      "Episode 19200, reward 0.87, epsilon 0.005\n",
      "Episode 19300, reward -0.59, epsilon 0.005\n",
      "Episode 19400, reward -0.64, epsilon 0.005\n",
      "Episode 19500, reward 0.89, epsilon 0.005\n",
      "Checkpoint saved at episode 19500\n"
     ]
    }
   ],
   "source": [
    "trained_model = train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "02db838d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_with_model(env, model, human_first=True):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    current_player = 0 if human_first else 1  # 0 = human, 1 = model\n",
    "    last_played = None\n",
    "\n",
    "    while not done:\n",
    "        print(f\"\\nCurrent player: {'Human' if current_player == 0 else 'AI'}\")\n",
    "        \n",
    "        if last_played:\n",
    "            last_char = last_played[-1].upper()\n",
    "            print(f\"Previous place: {last_played}\")\n",
    "            print(f\"Your place must START with: {last_char}\")\n",
    "        else:\n",
    "            print(\"First move - any place is valid\")\n",
    "        \n",
    "        if current_player == 0:\n",
    "            # Human turn\n",
    "            valid_moves = env.game.get_valid_moves()\n",
    "            if last_played:\n",
    "                print(f\"\\nValid moves starting with '{last_played[-1].upper()}': {valid_moves[:10]}\" + (\" ...\" if len(valid_moves) > 10 else \"\"))\n",
    "            else:\n",
    "                print(f\"\\nValid moves: {valid_moves[:10]}\" + (\" ...\" if len(valid_moves) > 10 else \"\"))\n",
    "            \n",
    "            move = input(\"Your move: \").strip().upper()\n",
    "            if move not in valid_moves:\n",
    "                print(f\"Illegal move! '{move}' is not valid.\")\n",
    "                print(\"Game over - you lose!\")\n",
    "                done = True\n",
    "                break\n",
    "            \n",
    "            action = env.game.places.index(move)\n",
    "            next_state, reward, done = env.step(action)\n",
    "            print(f\"✓ You played: {move}\")\n",
    "            last_played = move\n",
    "        else:\n",
    "            # AI turn\n",
    "            device = next(model.parameters()).device\n",
    "            state_tensor = torch.tensor(state, dtype=torch.float32, device=device)\n",
    "            \n",
    "            # Get valid moves from the game\n",
    "            valid_moves = env.game.get_valid_moves()\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                q = model(state_tensor)\n",
    "                valid_mask = torch.tensor(state[-env.n_actions:], dtype=torch.bool, device=device)\n",
    "                \n",
    "                # Only consider valid actions\n",
    "                q_masked = q.clone()\n",
    "                q_masked[~valid_mask] = float('-inf')\n",
    "                action = torch.argmax(q_masked).item()\n",
    "            \n",
    "            ai_place = env.game.places[action]\n",
    "            \n",
    "            # Check if AI's move is actually valid\n",
    "            if ai_place not in valid_moves:\n",
    "                print(f\"✓ AI plays: {ai_place}\")\n",
    "                print(f\"Illegal move! AI played '{ai_place}' but it should start with '{last_played[-1].upper()}'\")\n",
    "                print(\"Game over - Human wins!\")\n",
    "                done = True\n",
    "                break\n",
    "            \n",
    "            next_state, reward, done = env.step(action)\n",
    "            print(f\"✓ AI plays: {ai_place}\")\n",
    "            last_played = ai_place\n",
    "            \n",
    "            if done and reward < 0:\n",
    "                print(\"AI made an illegal move - you win!\")\n",
    "                break\n",
    "\n",
    "        state = next_state\n",
    "        current_player = 1 - current_player\n",
    "\n",
    "    print(\"\\nGame over!\")\n",
    "    if last_played:\n",
    "        print(f\"Final place: {last_played}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f37d97fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Current player: Human\n",
      "First move - any place is valid\n",
      "\n",
      "Valid moves: ['AFGHANISTAN', 'ALBANIA', 'ALGERIA', 'ANDORRA', 'ANGOLA', 'ANTIGUA AND BARBUDA', 'ARGENTINA', 'ARMENIA', 'AUSTRALIA', 'AUSTRIA'] ...\n",
      "✓ You played: AUSTRIA\n",
      "\n",
      "Current player: AI\n",
      "Previous place: AUSTRIA\n",
      "Your place must START with: A\n",
      "✓ AI plays: NORWAY\n",
      "\n",
      "Current player: Human\n",
      "Previous place: NORWAY\n",
      "Your place must START with: Y\n",
      "\n",
      "Valid moves starting with 'Y': ['NAMIBIA', 'NAURU', 'NEPAL', 'NETHERLANDS', 'NEW ZEALAND', 'NICARAGUA', 'NIGER', 'NIGERIA', 'NORTH KOREA', 'NORTH MACEDONIA']\n",
      "Illegal move! 'OKAY' is not valid.\n",
      "Game over - you lose!\n",
      "\n",
      "Game over!\n",
      "Final place: NORWAY\n"
     ]
    }
   ],
   "source": [
    "places = pd.read_csv(\"../../data/countries.csv\")[\"Country\"].values.tolist()\n",
    "env = AtlasEnv(places)\n",
    "play_with_model(env, trained_model, human_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1939a8e2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "701570e3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
